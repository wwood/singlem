#!/usr/bin/env python

__author__ = "Ben Woodcroft"
__copyright__ = "Copyright 2015-2016"
__credits__ = ["Ben Woodcroft"]
__license__ = "GPL3+"
__maintainer__ = "Ben Woodcroft"
__email__ = "b.woodcroft near uq.edu.au"
__status__ = "Development"

import argparse
import logging
import sys
import os
import tempfile
import subprocess
import math

sys.path = [os.path.join(os.path.dirname(os.path.realpath(__file__)),'..')] + sys.path

import singlem.sequence_database
import singlem.query_formatters
import singlem.pipe as pipe
from singlem.summariser import Summariser
from singlem.query_formatters import NameSequenceQueryDefinition,\
    NamedQueryDefinition
from singlem.strain_summariser import StrainSummariser
from singlem.sequence_classes import SeqReader
from singlem.metagenome_otu_finder import MetagenomeOtuFinder
from singlem.package_creator import PackageCreator
from singlem.otu_table_collection import OtuTableCollection, StreamingOtuTableCollection
from singlem.clusterer import Clusterer
from singlem.appraiser import Appraiser

DEFAULT_WINDOW_SIZE=60
ALMOST_90_PERCENT_ON_60BP = round(math.floor(0.97*60)/60, 3)

def seqs(args):
    if args.alignment_type == 'aa':
        is_protein_alignment = True
    elif args.alignment_type == 'dna':
        is_protein_alignment = False
    else:
        raise Exception("Unexpected alignment type '%s'" % args.alignment_type)

    # Read in the fasta Alignment
    protein_alignment = SeqReader().alignment_from_alignment_file(args.alignment)
    logging.info("Read in %i aligned protein sequences e.g. %s %s" % (
        len(protein_alignment),
        protein_alignment[0].name,
        protein_alignment[0].seq))

    best_position = MetagenomeOtuFinder().find_best_window(
        protein_alignment,
        args.window_size,
        is_protein_alignment)
    logging.info("Found best start position %i" % best_position)


def query(args):
    db = singlem.sequence_database.SequenceDatabase.acquire(args.db)
    query_sequence = args.query_sequence
    max_target_seqs = args.max_hits
    max_divergence = args.max_divergence
    output_style = args.otu_table_type
    query_otu_table = args.query_otu_table
    query_fasta = args.query_fasta
    if (query_otu_table and query_sequence) or \
        (query_otu_table and query_fasta) or \
        (query_sequence and query_fasta):
        raise Exception("Only one of --query_fasta, --query_otu_table and --query_sequence is allowable")

    if query_sequence:
        query_names = ['unnamed_sequence']
        query_sequences = [query_sequence]
    elif query_otu_table:
        query_names = []
        query_sequences = []
        otus = OtuTableCollection()
        otus.add_otu_table(open(query_otu_table))
        for e in otus:
            query_sequences.append(e.sequence)
            query_names.append(';'.join([e.sample_name,e.marker]))
    elif query_fasta:
        query_names = []
        query_sequences = []
        for name, seq, _ in SeqReader().readfq(open(query_fasta)):
            query_names.append(name)
            query_sequences.append(seq)
    else:
        raise Exception("No query option specified, cannot continue")

    # blast the query against the database, output as csv
    found_distances_and_names = []
    found_query_names = []
    found_query_sequences = []
    with tempfile.NamedTemporaryFile(prefix='singlem_query') as infile:
        for i, sequence in enumerate(query_sequences):
            infile.write(">%i\n" % i)
            infile.write(sequence+"\n")
        infile.flush()

        cmd = "blastn -task blastn -query '%s' -db '%s' -outfmt 6 -max_target_seqs %i" %\
            (infile.name, db.sequences_fasta_file, max_target_seqs)
        logging.debug("Running cmd %s" % cmd)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)

        last_index = None
        last_differences_and_names = []
        for line in iter(proc.stdout.readline,''):
            qseqid, sseqid, _, _, mismatch, gapopen, qstart, qend = line.strip().split("\t")[:8]
            index = int(qseqid)
            if last_index is None:
                # first run through loop
                last_index = index
            elif index != last_index:
                # changed query sequences, move to the next one
                found_query_names.append(query_names[last_index])
                found_query_sequences.append(query_sequences[last_index])
                found_distances_and_names.append(last_differences_and_names)
                last_index = index
                last_differences_and_names = []

            #TODO: check we haven't come up against the max_target_seqs barrier
            query_length = len(query_sequences[index].replace('-',''))
            divergence = int(mismatch) + int(gapopen) + (int(qstart)-1) + (query_length-int(qend))
            if divergence <= max_divergence:
                subject = db.extract_sequence_by_sseqid(sseqid)
                last_differences_and_names.append([divergence, subject])

        if last_index is not None:
            # finish off the last chunk
            found_query_names.append(query_names[last_index])
            found_query_sequences.append(query_sequences[last_index])
            found_distances_and_names.append(last_differences_and_names)

    if query_fasta:
        namedef = NamedQueryDefinition(found_query_names)
    else:
        namedef = NameSequenceQueryDefinition(found_query_names, found_query_sequences)

    if output_style == 'sparse':
        formatter = singlem.query_formatters.SparseResultFormatter(namedef, found_distances_and_names)
    elif output_style == 'dense':
        formatter = singlem.query_formatters.DenseResultFormatter(namedef, found_distances_and_names)
    else:
        raise Exception()
    formatter.write(sys.stdout)

if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    parser.add_argument('--debug', help='output debug information', action="store_true")
    parser.add_argument('--version', help='output version information and quit',  action='version', version=singlem.__version__)
    parser.add_argument('--quiet', help='only output errors', action="store_true")
    subparsers = parser.add_subparsers(title="Sub-commands", dest='subparser_name')

    pipe_description = 'Generate an OTU table from raw sequence data'
    pipe_parser = subparsers.add_parser('pipe',
                                        description=pipe_description,
                                        help=pipe_description,
                                        epilog=__author__)
    common_pipe_options = pipe_parser.add_argument_group('Common options')
    common_pipe_options.add_argument('--sequences', nargs='+', metavar='sequence_file(s)', help='nucleotide sequences to be searched')
    common_pipe_options.add_argument('--otu_table', metavar='filename', help='output OTU table')
    current_default = 1
    common_pipe_options.add_argument('--threads', type=int, metavar='num_threads', help='number of CPUS to use [default: %i]' % current_default, default=current_default)
    current_default = pipe.PPLACER_ASSIGNMENT_METHOD
    common_pipe_options.add_argument('--assignment_method', choices=(pipe.PPLACER_ASSIGNMENT_METHOD,
                                                             pipe.DIAMOND_ASSIGNMENT_METHOD,
                                                             pipe.DIAMOND_EXAMPLE_BEST_HIT_ASSIGNMENT_METHOD),
                                     help='method of assigning taxonomy to sequences. Using \'%s\' means than an example hit ID is given instead of a taxonomic classification [default: %s]' % (
                                         pipe.DIAMOND_EXAMPLE_BEST_HIT_ASSIGNMENT_METHOD,
                                         current_default),
                                     default=current_default)
    common_pipe_options.add_argument('--output_extras', action='store_true',
                                     help='give extra output for each sequence identified (e.g. the read(s) each OTU was generated from) [default: not set]',
                                     default=False)
    less_common_pipe_options = pipe_parser.add_argument_group('Less common options')
    less_common_pipe_options.add_argument('--archive_otu_table', metavar='filename', help='output OTU table in archive form for making DBs etc. [default: unused]')
    less_common_pipe_options.add_argument('--working_directory', metavar='directory', help='work in this directory [default: a temporary directory]')
    less_common_pipe_options.add_argument('--force', action='store_true', help='overwrite working directory if required [default: not set]')
    less_common_pipe_options.add_argument('--output_jplace', metavar='filename', help='Output a jplace format file for each singlem package to a file starting with this string, each with one entry per OTU. Requires \'%s\' as the --assignment_method [default: no used]' % pipe.PPLACER_ASSIGNMENT_METHOD)
    less_common_pipe_options.add_argument('--evalue', help='GraftM e-value cutoff [default: the GraftM default]')
    current_default = 96
    less_common_pipe_options.add_argument('--min_orf_length',
                                          metavar='length',
                                          help='When predicting ORFs require this many base pairs uninterrupted by a stop codon [default: %i]' % current_default,
                                          type=int, default=current_default)
    less_common_pipe_options.add_argument('--restrict_read_length',
                                          metavar='length',
                                          help='Only use this many base pairs at the start of each sequence searched [default: no restriction]',
                                          type=int)
    current_default = 28
    less_common_pipe_options.add_argument('--filter_minimum_protein',
                                          metavar='length',
                                          help='Ignore reads aligning in less than this many positions to each protein HMM [default: %i]' % current_default,
                                          type=int, default=current_default)
    current_default = 95
    less_common_pipe_options.add_argument('--filter_minimum_nucleotide',
                                          metavar='length',
                                          help='Ignore reads aligning in less than this many positions to each nucleotide HMM [default: %i]' % current_default,
                                          type=int, default=current_default)
    less_common_pipe_options.add_argument('--include_inserts', action='store_true',
                                          help='print the entirety of the sequences, not just the aligned nucleotides [default: not set]', default=False)
    less_common_pipe_options.add_argument('--known_otu_tables', nargs='+',
                                          help='OTU tables previously generated with trusted taxonomies for each sequence [default: unused]')
    less_common_pipe_options.add_argument('--singlem_packages', nargs='+', help='SingleM packages to use [default: use the default set]')
    less_common_pipe_options.add_argument('--no_assign_taxonomy', action='store_true',
                                          help='Do not assign any taxonomy except for those already known [default: not set]',
                                          default=False)
    less_common_pipe_options.add_argument('--known_sequence_taxonomy', metavar='FILE',
                                          help='A 2-column "sequence<tab>taxonomy" file specifying some sequences that have known taxonomy [default: unused]')

    seqs_description = 'Find the best window for a SingleM package'
    seqs_parser = subparsers.add_parser('seqs',
                                        description=seqs_description,
                                        help=seqs_description,
                                        epilog=__author__)
    seqs_parser.add_argument('--alignment', metavar='aligned_fasta', help="Protein sequences hmmaligned and converted to fasta format with seqmagick", required=True)
    seqs_parser.add_argument('--alignment_type', metavar='type', help="alignment is 'aa' or 'dna'", required=True)
    seqs_parser.add_argument('--window_size', metavar='INT', help='Number of nucleotides to use in continuous window', default=DEFAULT_WINDOW_SIZE, type=int)

    makedb_description = 'Create a searchable database from an OTU table'
    makedb_parser = subparsers.add_parser('makedb',
                                        description=makedb_description,
                                        help=makedb_description,
                                        epilog=__author__)
    makedb_parser.add_argument('--archive_otu_tables', nargs='+', help="Make a db from these archive tables")
    makedb_parser.add_argument('--otu_tables', nargs='+', help="Make a db from these OTU tables")
    makedb_parser.add_argument('--db_path', help="Name of database to create e.g. tundra.sdb", required=True)

    query_description = 'Find closely related sequences in a database'
    query_parser = subparsers.add_parser('query',
                                        description=query_description,
                                        help=query_description,
                                        epilog=__author__)
    query_parser.add_argument('--query_sequence', metavar='sequence', help="Sequence to use as a query")
    query_parser.add_argument('--query_otu_table', metavar='file', help="Query the database with all sequences in this OTU table")
    query_parser.add_argument('--query_fasta', metavar='file', help="Query the database with all sequences in this FASTA file")
    query_parser.add_argument('--db', help="Output from 'makedb' mode", required=True)
    query_parser.add_argument('--otu_table_type', help="Style of output table", default='sparse', choices=['dense','sparse'])
    query_parser.add_argument('--max_divergence', metavar='INT', help="Report sequences less than or equal to this divergence", default=4, type=int)
    query_parser.add_argument('--max_hits', help="--max_target_seqs parameter for blast", default=500, type=int)

    summarise_description = 'Summarise and transform OTU tables'
    summarise_parser = subparsers.add_parser('summarise',
                                        description=summarise_description,
                                        help=summarise_description,
                                        epilog=__author__)
    summarise_parser.add_argument('--input_archive_otu_tables', nargs='+', help="Summarise these tables")
    summarise_parser.add_argument('--input_otu_tables', nargs='+', help="Summarise these tables")
    summarise_parser.add_argument('--cluster', action='store_true', help="Apply sequence clustering to the OTU table")
    summarise_parser.add_argument('--cluster_id', type=float, help="Sequence clustering identity cutoff if --cluster is used", default=ALMOST_90_PERCENT_ON_60BP)
    summarise_parser.add_argument('--output_otu_table', help="Output combined OTU table to this file")
    summarise_parser.add_argument('--output_extras', action='store_true', help="Output extra information in the standard output OTU table", default=False)
    summarise_parser.add_argument('--krona', help="Name of krona file to generate")
    summarise_parser.add_argument('--strain_overview_table', help="Name of output strains table to generate")
    summarise_parser.add_argument('--unifrac', help="Output UniFrac format file")
    summarise_parser.add_argument('--clustered_output_otu_table', help="Output an OTU table with extra information about the clusters")
    summarise_parser.add_argument('--taxonomy', help="Restrict analysis to OTUs that have this taxonomy (exact taxonomy or more fully resolved)")
    summarise_parser.add_argument('--rarefied_output_otu_table', help="Output rarefied output OTU table, where each gene and sample combination is rarefied")
    summarise_parser.add_argument('--number_to_choose', type=int, help="Rarefy using this many sequences. Sample/gene combinations with an insufficient number of sequences are ignored with a warning [default: maximal number such that all are sufficient]")

    create_description = 'Create a SingleM package'
    create_parser = subparsers.add_parser('create',
                                        description=create_description,
                                        help=create_description,
                                        epilog=__author__)
    create_parser.add_argument('--input_graftm_package', metavar="PATH", help="input package", required=True)
    create_parser.add_argument('--output_singlem_package', metavar="PATH", help="output package", required=True)
    create_parser.add_argument('--hmm_position', metavar="INTEGER", help="position in the GraftM alignment HMM where the SingleM window starts", required=True, type=int)
    create_parser.add_argument('--force', action='store_true', help='overwrite output path if it already exists')

    appraise_description = 'How much of the metagenome do the genomes account for?'
    appraise_parser = subparsers.add_parser('appraise',
                                        description=appraise_description,
                                        help=appraise_description,
                                        epilog=__author__)
    appraise_parser.add_argument('--metagenome_otu_tables', nargs='+', help="output of 'pipe' run on metagenomes", required=True)
    appraise_parser.add_argument('--genome_otu_tables', nargs='+', help="output of 'pipe' run on genomes", required=True)
    appraise_parser.add_argument('--imperfect', action='store_true', help="use sequence searching to account for genomes that are similar to those found in the metagenome", default=False)
    appraise_parser.add_argument('--sequence_identity', type=float, help="sequence identity cutoff to use if --imperfect is specified", default=ALMOST_90_PERCENT_ON_60BP)
    appraise_parser.add_argument('--accounted_for_otu_table', help="output OTU table of accounted-for populations", default=None)
    appraise_parser.add_argument('--unaccounted_for_otu_table', help="Output OTU table of populations not accounted for", default=None)

    if (len(sys.argv) == 1 or sys.argv[1] == '-h' or sys.argv == '--help'):
        print ''
        print '                ...::: SingleM v' + singlem.__version__ + ' :::...'''
        print '\n  General usage:'
        print '    pipe         -> %s' % pipe_description
        print '    summarise    -> %s' % summarise_description
        print '    appraise     -> %s' % appraise_description

        print '\n  Databases:'
        print '    makedb       -> %s' % makedb_description
        print '    query        -> %s' % query_description

        print '\n  Package creation:'
        print '    seqs         -> %s' % seqs_description
        print '    create       -> %s' % create_description

        print '\n  Use: singlem <command> -h for command-specific help\n'
        sys.exit(0)
    else:
        args = parser.parse_args()

    if args.debug:
        loglevel = logging.DEBUG
    elif args.quiet:
        loglevel = logging.ERROR
    else:
        loglevel = logging.INFO
    logging.basicConfig(level=loglevel, format='%(asctime)s %(levelname)s: %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')

    if args.subparser_name == 'seqs':
        seqs(args)
    elif args.subparser_name=='pipe':
        if not args.otu_table and not args.archive_otu_table:
            raise Exception("At least one of --otu_table or --archive_otu_table must be specified")
        if args.output_jplace and args.assignment_method != pipe.PPLACER_ASSIGNMENT_METHOD:
            raise Exception("If --output_jplace is specified, then --assignment_method must be set to %s" % pipe.PPLACER_ASSIGNMENT_METHOD)
        if args.output_jplace and args.known_otu_tables:
            raise Exception("Currently --output_jplace and --known_otu_tables are incompatible")
        if args.output_jplace and args.no_assign_taxonomy:
            raise Exception("Currently --output_jplace and --no_assign_taxonomy are incompatible")
        if args.known_sequence_taxonomy and not args.no_assign_taxonomy:
            raise Exception(
                "Currently --known_sequence_taxonomy requires --no_assign_taxonomy to be set also")
        singlem.pipe.SearchPipe().run(
            sequences = args.sequences,
            otu_table = args.otu_table,
            archive_otu_table = args.archive_otu_table,
            threads = args.threads,
            known_otu_tables = args.known_otu_tables,
            assignment_method = args.assignment_method,
            output_jplace = args.output_jplace,
            evalue = args.evalue,
            min_orf_length = args.min_orf_length,
            restrict_read_length = args.restrict_read_length,
            filter_minimum_protein = args.filter_minimum_protein,
            filter_minimum_nucleotide = args.filter_minimum_nucleotide,
            output_extras = args.output_extras,
            include_inserts = args.include_inserts,
            working_directory = args.working_directory,
            force = args.force,
            singlem_packages = args.singlem_packages,
            window_size=DEFAULT_WINDOW_SIZE,
            assign_taxonomy = not args.no_assign_taxonomy,
            known_sequence_taxonomy = args.known_sequence_taxonomy)

    elif args.subparser_name=='makedb':
        if not args.otu_tables and not args.archive_otu_tables:
            raise Exception("Making a database requires input OTU tables or archive tables")
        otus = StreamingOtuTableCollection()
        if args.otu_tables:
            for o in args.otu_tables:
                otus.add_otu_table_file(o)
        if args.archive_otu_tables:
            for o in args.archive_otu_tables:
                otus.add_archive_otu_table_file(o)
        singlem.sequence_database.SequenceDatabase.create_from_otu_table\
            (args.db_path, otus)
    elif args.subparser_name=='query':
        query(args)
    elif args.subparser_name == 'summarise':
        num_output_types = 0
        if args.strain_overview_table: num_output_types += 1
        if args.krona: num_output_types += 1
        if args.unifrac: num_output_types += 1
        if args.output_otu_table: num_output_types += 1
        if args.clustered_output_otu_table: num_output_types += 1
        if args.rarefied_output_otu_table: num_output_types += 1
        if num_output_types != 1:
            raise Exception("Exactly 1 output type must be specified, sorry, %i were provided" % num_output_types)
        if not args.input_otu_tables and not args.input_archive_otu_tables:
            raise Exception("Summary requires input OTU tables or archive tables")

        otus = OtuTableCollection()
        otus.set_target_taxonomy_by_string(args.taxonomy)
        if args.input_otu_tables:
            for o in args.input_otu_tables:
                otus.add_otu_table(open(o))
        if args.input_archive_otu_tables:
            for o in args.input_archive_otu_tables:
                otus.add_archive_otu_table(open(o))

        if args.cluster:
            logging.info("Clustering OTUs with clustering identity %f.." % args.cluster_id)
            o2 = OtuTableCollection()
            o2.otu_table_objects = [list(Clusterer().each_cluster(otus, args.cluster_id))]
            otus = o2
            logging.info("Finished clustering")

        if args.krona:
            Summariser.summarise(
                table_collection = otus,
                krona_output = args.krona)
        elif args.unifrac:
            Summariser.write_unifrac_format_file(
                table_collection = otus,
                unifrac_output_prefix = args.unifrac)
        elif args.output_otu_table:
            Summariser.write_otu_table(
                table_collection = otus,
                output_table_io = open(args.output_otu_table,'w'),
                output_extras = args.output_extras)
        elif args.strain_overview_table:
            StrainSummariser().summarise_strains(
                table_collection = otus,
                output_table_io = open(args.strain_overview_table,'w'))
        elif args.clustered_output_otu_table:
            if not args.cluster:
                raise Exception("If --clustered_output_otu_table is set, then clustering (--cluster) must be applied")
            Summariser.write_clustered_otu_table(
                table_collection = otus,
                output_table_io = open(args.clustered_output_otu_table,'w'))
        elif args.rarefied_output_otu_table:
            Summariser.write_rarefied_otu_table(
                table_collection = otus,
                output_table_io = open(args.rarefied_output_otu_table,'w'),
                number_to_choose = args.number_to_choose)

        else: raise Exception("Programming error")
        logging.info("Finished")

    elif args.subparser_name == 'create':
        PackageCreator().create(input_graftm_package = args.input_graftm_package,
                                output_singlem_package = args.output_singlem_package,
                                hmm_position = args.hmm_position,
                                force = args.force)
    elif args.subparser_name == 'appraise':
        appraiser = Appraiser()

        genomes = OtuTableCollection()
        for table in args.genome_otu_tables:
            genomes.add_otu_table(open(table))
        metagenomes = OtuTableCollection()
        for table in args.metagenome_otu_tables:
            metagenomes.add_otu_table(open(table))

        app = appraiser.appraise(genome_otu_table_collection=genomes,
                                 metagenome_otu_table_collection=metagenomes,
                                 sequence_identity=(args.sequence_identity if args.imperfect else None))

        if args.accounted_for_otu_table:
            accounted_for_otu_table_io = open(args.accounted_for_otu_table,'w')
        else:
            accounted_for_otu_table_io = None
        if args.unaccounted_for_otu_table:
            unaccounted_for_otu_table_io = open(args.unaccounted_for_otu_table,'w')
        else:
            unaccounted_for_otu_table_io = None

        appraiser.print_appraisal(app,
                                  accounted_for_otu_table_io=accounted_for_otu_table_io,
                                  unaccounted_for_otu_table_io=unaccounted_for_otu_table_io)
        if args.accounted_for_otu_table: accounted_for_otu_table_io.close()
        if args.unaccounted_for_otu_table: unaccounted_for_otu_table_io.close()
    else:
        raise Exception("Programming error")
