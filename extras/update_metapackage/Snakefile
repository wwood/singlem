#############
### Setup ###
#############
"""
Steps:

pixi run -e update-metapackage snakemake --profile aqua --configfile config-R226.yaml

NOTE! This file is being migrated to pixi, but we aren't there yet, some rules haven't been migrated yet.
"""

import pandas as pd
import os

configfile: "config.yaml"

hmms_and_names = pd.read_csv("hmms_and_names", sep="\t").set_index("name", drop=False)
singlem_bin = "pixi run --manifest-path ../../pixi.toml ../../singlem/main.py"

output_dir = config["output_dir"]
os.makedirs(output_dir, exist_ok=True)
logs_dir = output_dir + "/logs"
os.makedirs(logs_dir, exist_ok=True)
benchmark_dir = output_dir + "/benchmarks"
os.makedirs(benchmark_dir, exist_ok=True)

checkm2_output_dirs = config['checkm2_output_dirs'] if 'checkm2_output_dirs' in config else None # not needed for R220 and beyond (actually R220 was missing some data, so using it there too)
taxonomy_database_name = config["taxonomy_database_name"]
taxonomy_database_version = config["taxonomy_database_version"]

# TODO: This finding genomes at the start is slow, and is run each time
# snakemake is started up in a queued job. Would be better to do this once, and
# then reuse cached results.
print("Finding genomes...")
if 'compressed_genome_data' in config and config['compressed_genome_data']:
    genome_paths = [os.path.join(d, f) for d, _, fs in os.walk(config["gtdb_protein_faa_reps"], followlinks=True) for f in fs if f.endswith(".faa.gz")]
else:
    genome_paths = [os.path.join(d, f) for d, _, fs in os.walk(config["gtdb_protein_faa_reps"], followlinks=True) for f in fs if f.endswith(".faa")]
genomes = [g.removeprefix(config["gtdb_protein_faa_reps"]).removeprefix("/") for g in genome_paths]

size_genome_splits = 50
n_genome_splits = len(genomes) // size_genome_splits + 1

zenodo_backpack = os.path.join(output_dir, config["new_metapackage"]+'.zb.tar.gz')
zenodo_backpack_version = config['new_version_number']

# genomes = genomes[:50]+genomes[(len(genomes)-50):] # debug, so have archaea and bacteria
print("Found {} genomes.".format(len(genomes)))
# Write genomes to a file for use in rules
genome_list_file = os.path.join(output_dir, "genome_list.txt")

# Sort genomes for consistent ordering
genomes.sort()

# Read existing file if it exists and compare
new_content = '\n'.join(genomes)
write_file = True
if os.path.exists(genome_list_file):
    with open(genome_list_file, 'r') as f:
        existing_content = f.read().strip()
    if existing_content == new_content:
        write_file = False

# Only write if content is different
if write_file:
    with open(genome_list_file, 'w') as f:
        f.write(new_content)
    print("Wrote genome list to {}".format(genome_list_file))
else:
    print("Genome list unchanged, skipping write to {}".format(genome_list_file))

if not "max_threads" in config: config["max_threads"] = 8

wildcard_constraints:
    genome = r"(archaea|bacteria)/\w+.\d+_protein.faa"

def get_mem_mb(wildcards, threads, attempt):
    return 8 * 1000 * threads * attempt

rule all:
    input:
        zenodo_backpack,
        output_dir + "/sra/done"

rule zenodo_backpack:
    input:
        metapackage = output_dir + "/metapackage/" + config["new_metapackage"],
    output:
        zenodo_backpack=zenodo_backpack,
        done=touch(output_dir + "/zenodo_backpack.done")
    conda:
        "envs/zenodo_backpack.yml"
    shell:
        "zenodo_backpack create --input-directory {input.metapackage} --output-file {output.zenodo_backpack} --data-version {zenodo_backpack_version}"

####################
### HMM searches ###
####################
rule pfam_search:
    input:
        genome_list=genome_list_file  # Add this as proper input
    output:
        done=touch(output_dir + "/pfam_search.done")
    params:
        pfams = config["pfams"],
        output_dir = output_dir + "/hmmsearch/pfam/",
        compress_flag = "--compressed" if config.get("compressed_genome_data", False) else "",
    threads:
        config["max_threads"]
    resources:
        mem_mb = 16*1024,
        runtime = 24*60,
    benchmark:
        benchmark_dir + "/pfam_search.benchmark"
    log:
        logs_dir + "/pfam_search.log"
    shell:
        """
        pixi run -e pfam-search python scripts/pfam_search.py \
            --pfams {config[pfams]} \
            --genome-ids-file {input.genome_list} \
            --fasta-base {config[gtdb_protein_faa_reps]} \
            --output-dir {params.output_dir} \
            --threads {threads} \
            {params.compress_flag} 2>{log}
        """

rule tigrfam_search:
    input:
        genome_list=genome_list_file  # Add this as proper input
    output:
        done=touch(output_dir + "/tigrfam_search.done")
    params:
        tigrfams = config["tigrfams"],
        output_dir=output_dir + "/hmmsearch/tigrfam/",
        compress_flag = "--compressed" if config.get("compressed_genome_data", False) else "",
    threads:
        config["max_threads"]
    resources:
        mem_mb = 16*1024,
        runtime = 24*60,
    benchmark:
        benchmark_dir + "/tigrfam_search.benchmark"
    log:
        logs_dir + "/tigrfam_search.log"
    shell:
        """
        pixi run -e tigrfam-search python scripts/tigrfam_search.py \
            --tigrfams {config[tigrfams]} \
            --genome-ids-file {input.genome_list} \
            --fasta-base {config[gtdb_protein_faa_reps]} \
            --output-dir {params.output_dir} \
            --threads {threads} \
            {params.compress_flag} 2>{log}
        """

rule get_matches_no_dup:
    input:
        output_dir + "/pfam_search.done",
        output_dir + "/tigrfam_search.done",
    output:
        done=touch(output_dir + "/get_matches_no_dup.done")
    params:
        pfam_search_directory = output_dir + "/hmmsearch/pfam/",
        tigrfam_search_directory = output_dir + "/hmmsearch/tigrfam/",
        hmms_and_names = "hmms_and_names",
        output_dir = output_dir + "/hmmsearch/matches",
        logs_dir = logs_dir + "/hmmsearch/matches",
        genome_list_file = genome_list_file,
    threads:
        config["max_threads"]
    resources:
        mem_mb = 16*1024,
        runtime = 4*60,
    benchmark:
        benchmark_dir + "/get_matches_no_dup.benchmark"
    log:
        logs_dir + "/get_matches_no_dup.log"
    shell:
        """
        pixi run -e singlem python scripts/get_matches_no_dup_all.py \
            --genome-ids-file {params.genome_list_file} \
            --pfam-search-directory {params.pfam_search_directory} \
            --tigrfam-search-directory {params.tigrfam_search_directory} \
            --hmms-and-names {params.hmms_and_names} \
            --output-dir {params.output_dir} \
            --logs-dir {params.logs_dir} \
            --threads {threads} 2> {log}
        """

rule mfqe:
    input:
        done=output_dir + "/get_matches_no_dup.done",
        genome_list=genome_list_file  # Add this as proper input
    params:
        fam_directory = output_dir + "/hmmsearch/matches",
        output_dir = output_dir + "/hmmsearch/matches",
        logs_dir = logs_dir + "/mfqe",
        compressed_flag = "--compressed" if config.get("compressed_genome_data", False) else ""  # Move this to params
    output:
        done=touch(output_dir + "/mfqe.done")
    threads: 1
    resources:
        mem_mb = 8*1024,
        runtime = 24*60,
    benchmark:
        benchmark_dir + "/mfqe.benchmark"
    log:
        logs_dir + "/mfqe.log"
    shell:
        '''
        pixi run -e singlem python scripts/mfqe_all.py \
            --genome-ids-file {input.genome_list} \
            --fam-directory {params.fam_directory} \
            --gtdb-protein-faa-reps {config[gtdb_protein_faa_reps]} \
            --output-dir {params.output_dir} \
            --logs-dir {params.logs_dir} \
            --threads {threads} \
            {params.compressed_flag} 2> {log}
        '''

########################
### Package creation ###
########################
rule transpose_hmms_with_sequences:
    input:
        output_dir + "/mfqe.done",
    output:
        done = touch(output_dir + "/transpose_hmms_with_sequences.done"),
    params:
        output_dir = directory(output_dir + "/hmmseq/"), # output is file for each spkg
        hmms_and_names = "hmms_and_names",
        matches_dir = output_dir + "/hmmsearch/matches",
        logs_dir = logs_dir + "/transpose_hmms_with_sequences",
        genome_list=genome_list_file,
    threads:
        config["max_threads"]
    resources:
        mem_mb = 16*1024,
        runtime = 24*60,
    benchmark:
        benchmark_dir + "/transpose_hmms_with_sequences.benchmark"
    log:
        logs_dir + "/transpose_hmms_with_sequences.log"
    shell:
        """
        pixi run -e transpose python scripts/transpose_hmms_with_sequences_all.py \
            --genome-ids-list {params.genome_list} \
            --matches-dir {params.matches_dir} \
            --output-dir {params.output_dir} \
            --gtdb-bac-tax {config[gtdb_bac_tax]} \
            --gtdb-arc-tax {config[gtdb_arc_tax]} \
            --hmms-and-names {params.hmms_and_names} \
            --logs-dir {params.logs_dir} \
            --threads {threads} 2> {log}
        """

rule concatenate_seqs_and_taxonomies:
    input:
        done = output_dir + "/transpose_hmms_with_sequences.done",
    output:
        done = touch(output_dir + "/concatenate_seqs_and_taxonomies.{spkg}.done"),
    params:
        hmmseq_dir = output_dir + "/hmmseq/",
        concat_dir = output_dir + "/hmmseq_concat/",
        spkg_seq = output_dir + "/hmmseq_concat/{spkg}.faa",
        spkg_tax = output_dir + "/hmmseq_concat/{spkg}_taxonomy.tsv",
    threads: 1
    resources:
        mem_mb = 16*1024,
        runtime = 24*60,
    shell:
        "mkdir -p {params.concat_dir} && find {params.hmmseq_dir} |grep -F {wildcards.spkg} |grep .faa$ |parallel -j1 --ungroup cat {{}} >{params.spkg_seq} && find {params.hmmseq_dir} |grep -F {wildcards.spkg} |grep _taxonomy.tsv$ |parallel -j1 --ungroup cat {{}} >{params.spkg_tax}"

# Rule not part of the main workflow here.
rule all_concatenate_seqs_and_taxonomies:
    input:
        expand(output_dir + "/concatenate_seqs_and_taxonomies.{spkg}.done", spkg = hmms_and_names.index)
    output:
        touch(output_dir + "/concatenate_seqs_and_taxonomies.done")

rule create_SingleM_packages:
    input:
        output_dir + "/concatenate_seqs_and_taxonomies.{spkg}.done",
    output:
        directory(output_dir + "/packages/{spkg}.spkg")
    params:
        singlem = singlem_bin,
        hmms_and_names = "hmms_and_names",
        uniprot_seq = config["uniprot_seq"],
        uniprot_tax = config["uniprot_tax"],
        spkg = config["metapackage"] + "/{spkg}.spkg",
        spkg_seq = output_dir + "/hmmseq_concat/{spkg}.faa",
        spkg_tax = output_dir + "/hmmseq_concat/{spkg}_taxonomy.tsv",
        spkg_name = lambda wildcards: hmms_and_names.loc[wildcards.spkg, "name_without_number"]
    threads: 1
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    log:
        logs_dir + "/packages/{spkg}.log"
    conda:
        "../../singlem.yml"
    shell:
        "{params.singlem} regenerate "
        "--input-singlem-package {params.spkg} "
        "--input-sequences {params.spkg_seq} "
        "--input-taxonomy {params.spkg_tax} "
        "--euk-sequences {params.uniprot_seq} "
        "--euk-taxonomy {params.uniprot_tax} "
        "--output-singlem-package {output} "
        "--sequence-prefix {params.spkg_name}~ "
        "&> {log}"

############################
### Metapackage creation ###
############################
rule create_draft_SingleM_metapackage:
    input:
        packages = expand(output_dir + "/packages/{spkg}.spkg", spkg = hmms_and_names.index)
    output:
        directory(output_dir + "/draft_metapackage.smpkg")
    params:
        singlem = singlem_bin
    threads:
        config["max_threads"]
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    log:
        logs_dir + "/draft_metapackage.log"
    conda:
        "../../singlem.yml"
    shell:
        "{params.singlem} metapackage "
        "--singlem-packages {input.packages} "
        "--no-nucleotide-sdb "
        "--no-taxon-genome-lengths "
        "--metapackage {output} "
        "--threads {threads} "
        "&> {log}"

rule SingleM_transcripts_genome_lists:
    input:
        dir = config["gtdb_protein_fna_reps"],
    output:
        dir = directory(output_dir + "/transcript_lists"),
    threads:
        1
    resources:
        mem_mb = get_mem_mb,
        runtime = 5*60,
    params:
        size = size_genome_splits,
    shell:
        "mkdir -p {output.dir} && "
        "find -L {input.dir} \\( -name '*.fa.gz' -o -name '*.fna.gz' \\) "
        "| parallel --pipe -N{params.size} 'cat > {output.dir}/{{#}}.txt' "

rule SingleM_transcripts:
    input:
        dir = config["gtdb_protein_fna_reps"],
        metapackage = output_dir + "/draft_metapackage.smpkg",
        lists = output_dir + "/transcript_lists",
    output:
        otu_table = output_dir + "/transcripts/{n}.otu_table.tsv",
    threads: 1
    resources:
        mem_mb = get_mem_mb,
        runtime = 10*60,
    params:
        singlem = singlem_bin,
        input_files = output_dir + "/transcript_lists/{n}.txt"
    conda:
        "../../singlem.yml"
    log:
        logs_dir + "/transcripts/{n}.log"
    shell:
        "{params.singlem} pipe "
        "--forward $(cat {params.input_files} | tr '\n' ' ') "
        "--metapackage {input.metapackage} "
        "--otu-table {output.otu_table} "
        "--no-assign-taxonomy "
        "&> {log} "

rule collect_SingleM_transcripts:
    input:
        expand(output_dir + "/transcripts/{n}.otu_table.tsv", n = [i + 1 for i in range(n_genome_splits)])
    output:
        touch(output_dir + "/transcripts/done")

rule assign_taxonomy:
    input:
        touch = output_dir + "/transcripts/done"
    output:
        output_dir + "/taxonomy/transcripts.otu_table.tsv"
    threads: 1
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    params:
        bac_metadata = config["gtdb_bac_metadata"],
        arc_metadata = config["gtdb_arc_metadata"],
        dir = output_dir + "/transcripts",
    conda:
        "../../singlem.yml"
    log:
        logs_dir + "/taxonomy.log"
    shell:
        "../assign_gtdb_taxonomy_to_gtdb_genomes.py "
        "--otu-table-list <( find {params.dir} -name '*.otu_table.tsv' ) "
        "--gtdb-bac {params.bac_metadata} "
        "--gtdb-arc {params.arc_metadata} "
        "> {output} "
        "2> {log}"

rule make_sdb:
    input:
        output_dir + "/taxonomy/transcripts.otu_table.tsv"
    output:
        directory(output_dir + "/taxonomy/transcripts.sdb")
    threads:
        config["max_threads"]
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    params:
        singlem = singlem_bin
    log:
        logs_dir + "/taxonomy/makedb.log"
    conda:
        "../../singlem.yml"
    shell:
        "{params.singlem} makedb "
        "--otu-table {input} "
        "--db {output} "
        "--threads {threads} "
        "&> {log}"

rule create_SingleM_metapackage:
    input:
        packages = expand(output_dir + "/packages/{spkg}.spkg", spkg = hmms_and_names.index),
        sdb = output_dir + "/taxonomy/transcripts.sdb",
        genome_sizes = output_dir + "/gtdb_mean_genome_sizes.tsv"
    output:
        directory(output_dir + "/metapackage/" + config["new_metapackage"])
    params:
        singlem = singlem_bin
    threads:
        config["max_threads"]
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    log:
        logs_dir + "/metapackage.log"
    conda:
        "../../singlem.yml"
    shell:
        "{params.singlem} metapackage "
        "--singlem-packages {input.packages} "
        "--nucleotide-sdb {input.sdb} "
        "--taxon-genome-lengths {input.genome_sizes} "
        "--metapackage {output} "
        "--threads {threads} "
        "--taxonomy-database-name '{taxonomy_database_name}' "
        "--taxonomy-database-version '{taxonomy_database_version}' "
        "&> {log}"

#######################################
### Mean genome size table creation ###
#######################################
rule checkm2_gather: # In future presumably this data will be available form the GTDB metadata
    input:
        checkm2_output_dirs = checkm2_output_dirs if checkm2_output_dirs else [],
    output:
        gtdb_checkm2_grepped = output_dir + "/gtdb_checkm2_grepped.tsv",
        done = touch(output_dir + "/checkm2_gather.done"),
    log:
        logs_dir + "/checkm2_gather.log"
    shell:
        # grep -hv Completeness {input.checkm2_output_dirs}/*.checkm2/quality_report.tsv
        "find {input.checkm2_output_dirs} -name '*quality_report.tsv' |parallel -N50 grep -hv Completeness {{}} > {output.gtdb_checkm2_grepped} 2> {log}"

rule calculate_mean_genome_size:
    input:
        # Only need these inputs if checkm2 isn't in the metadata
        gtdb_checkm2_grepped = output_dir + "/gtdb_checkm2_grepped.tsv" if checkm2_output_dirs else [],
        done = output_dir + "/checkm2_gather.done" if checkm2_output_dirs else [],
    output:
        output_dir + "/gtdb_mean_genome_sizes.tsv"
    params:
        gtdb_bac_metadata = config["gtdb_bac_metadata"],
        gtdb_arc_metadata = config["gtdb_arc_metadata"],
        checkm2_grep_arg = '--checkm2-grep ' + output_dir + "/gtdb_checkm2_grepped.tsv" if checkm2_output_dirs else ''
    threads: 8
    resources:
        mem_mb = get_mem_mb,
        runtime = '1h',
    log:
        logs_dir + "/calculate_mean_genome_size.log"
    conda:
        "envs/calculate_mean_genome_size.yml"
    shell:
        """
        scripts/genome_size_from_gtdb.py {params.checkm2_grep_arg} --gtdb-bac-metadata {params.gtdb_bac_metadata} --gtdb-ar-metadata {params.gtdb_arc_metadata} > {output} 2> {log}
        """

###########################
### Metapackage testing ###
###########################
rule test_metapackage:
    input:
        metapackge = output_dir + "/metapackage/" + config["new_metapackage"],
        sra_1 = lambda wildcards: config["sra_seqs_1"][wildcards.sra],
        sra_2 = lambda wildcards: config["sra_seqs_2"][wildcards.sra],
    output:
        table = output_dir + "/sra/{sra}_new.otu_table.tsv",
        archive_table = output_dir + "/sra/{sra}_new.otu_table.json",
        profile = output_dir + "/sra/{sra}_new.otu_table.condensed.tsv",
        profile_krona = output_dir + "/sra/{sra}_new.otu_table.condensed.krona.html"
    params:
        singlem = singlem_bin
    threads: 8
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    log:
        logs_dir + "/sra/{sra}_new_test.log"
    conda:
        "../../singlem.yml"
    shell:
        "{params.singlem} pipe "
        "--forward {input.sra_1} "
        "--reverse {input.sra_2} "
        "--metapackage {input.metapackge} "
        "--otu-table {output.table} "
        "--archive-otu-table {output.archive_table} "
        "-p {output.profile} "
        "--taxonomic-profile-krona {output.profile_krona} "
        "--threads {threads} "
        "&> {log}"

rule test_old_metapackage:
    input:
        sra_1 = lambda wildcards: config["sra_seqs_1"][wildcards.sra],
        sra_2 = lambda wildcards: config["sra_seqs_2"][wildcards.sra],
    output:
        table = output_dir + "/sra/{sra}_old.otu_table.tsv",
        archive_table = output_dir + "/sra/{sra}_old.otu_table.json",
        profile = output_dir + "/sra/{sra}_old.otu_table.condensed.tsv",
        profile_krona = output_dir + "/sra/{sra}_old.otu_table.condensed.krona.html"
    params:
        singlem = singlem_bin,
        metapackge = config["metapackage"],
    threads: 8
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    log:
        logs_dir + "/sra/{sra}_old_test.log"
    conda:
        "../../singlem.yml"
    shell:
        "{params.singlem} pipe "
        "--forward {input.sra_1} "
        "--reverse {input.sra_2} "
        "--metapackage {params.metapackge} "
        "--otu-table {output.table} "
        "--archive-otu-table {output.archive_table} "
        "-p {output.profile} "
        "--taxonomic-profile-krona {output.profile_krona} "
        "--threads {threads} "
        "&> {log}"

rule sra_fractions_assigned_to_each_level:
    input:
        profile = output_dir + "/sra/{sra}_{version}.otu_table.condensed.tsv",
    output:
        breakdown = output_dir + "/sra/{sra}_{version}.taxonomic_coverage.tsv"
    threads: 1
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    conda:
        "../../singlem.yml"
    shell:
        "{singlem_bin} summarise "
        "--input-taxonomic-profile {input.profile} "
        "--output-taxonomic-level-coverage {output.breakdown} "

rule sra_prokaryotic_fraction:
    input:
        profile = output_dir + "/sra/{sra}_{version}.otu_table.condensed.tsv",
        sra_1 = lambda wildcards: config["sra_seqs_1"][wildcards.sra],
        sra_2 = lambda wildcards: config["sra_seqs_2"][wildcards.sra],
    output:
        smf = output_dir + "/sra/{sra}_{version}.smf.tsv"
    threads: 1
    resources:
        mem_mb = get_mem_mb,
        runtime = 48*60,
    params:
        metapackge = lambda wildcards: config["metapackage"] if wildcards.version == "old" else output_dir + "/metapackage/" + config["new_metapackage"],
    conda:
        "../../singlem.yml"
    log:
        logs_dir + "/sra/{sra}_{version}_smf.log"
    shell:
        "{singlem_bin} prokaryotic_fraction "
        "--metapackage {params.metapackge} "
        "--forward {input.sra_1} "
        "--reverse {input.sra_2} "
        "-p {input.profile} "
        "> {output.smf} "
        "2> {log} "

rule check_tests:
    input:
        expand(output_dir + "/sra/{sra}_new.otu_table.tsv", sra=config["sra_seqs_1"]),
        expand(output_dir + "/sra/{sra}_old.otu_table.tsv", sra=config["sra_seqs_1"]),
        expand(output_dir + "/sra/{sra}_{version}.taxonomic_coverage.tsv", sra=config["sra_seqs_1"], version=["new", "old"]),
        expand(output_dir + "/sra/{sra}_{version}.smf.tsv", sra=config["sra_seqs_1"], version=["new", "old"]),
    output:
        touch(output_dir + "/sra/done")
